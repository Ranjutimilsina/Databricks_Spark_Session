{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ae5c61-4e40-45e3-b0e7-4f689f99c19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac79eb3-c421-4e22-94eb-f3cb9cbe5e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Drop the sales_data table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS sales_data\")\n",
    "    # Drop the sales_data table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS product_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8adb13e3-7fb6-4b03-89f8-3772cfbe399e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: 'sales_data'"
     ]
    }
   ],
   "source": [
    "def sales_data_collector_api(spark, text_file_path):\n",
    "    # Read the sales data from the text file with headers and '|' delimiter\n",
    "    df_sales = spark.read.option(\"delimiter\", \"|\").option(\"header\", \"true\").csv(text_file_path)\n",
    "    \n",
    "    # Write the sales data to a partitioned Hive table (by sale_date) in Parquet format\n",
    "    df_sales.write.partitionBy(\"sale_date\").format(\"parquet\").saveAsTable(\"sales_data\")\n",
    "    \n",
    "    return \"sales_data\"  \n",
    "\n",
    "# Call the function to process the sales data\n",
    "sales_data_collector_api(spark, \"dbfs:/FileStore/shared_uploads/timilsina.ra@northeastern.edu/sales_data-5.txt\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af16fedc-3f07-4266-9923-b7da14574e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: 'product_data'"
     ]
    }
   ],
   "source": [
    "def product_data_collector_api(spark, text_file_path):\n",
    "    # Read the product data from the text file with headers and '|' delimiter\n",
    "    df_product = spark.read.option(\"delimiter\", \"|\").option(\"header\", \"true\").csv(text_file_path)\n",
    "    \n",
    "    # Convert the DataFrame into Parquet format and save it to a Hive table\n",
    "    df_product.write.format(\"parquet\").saveAsTable(\"product_data\")\n",
    "    \n",
    "    return \"product_data\"  \n",
    "\n",
    "# Call the function to process the product data\n",
    "product_data_collector_api(spark, \"dbfs:/FileStore/shared_uploads/timilsina.ra@northeastern.edu/product_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786cc1ba-fbcd-4d84-aec2-1f6f704374ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: 'buyers_s8_not_iphone'"
     ]
    }
   ],
   "source": [
    "def data_preparation_api(spark, product_hive_table, sales_hive_table, target_hive_table):\n",
    "    # Load the product and sales data from Hive\n",
    "    df_product = spark.table(product_hive_table)\n",
    "    df_sales = spark.table(sales_hive_table)\n",
    "    \n",
    "    # Get product IDs for S8 and iPhone\n",
    "    s8_product_id = df_product.filter(df_product.product_name == \"S8\").select(\"product_id\").collect()[0][0]\n",
    "    iphone_product_id = df_product.filter(df_product.product_name == \"iPhone\").select(\"product_id\").collect()[0][0]\n",
    "    \n",
    "    # Get the list of buyers who bought S8\n",
    "    s8_buyers = df_sales.filter(df_sales.product_id == s8_product_id).select(\"buyer_id\").distinct()\n",
    "    \n",
    "    # Get the list of buyers who bought iPhone\n",
    "    iphone_buyers = df_sales.filter(df_sales.product_id == iphone_product_id).select(\"buyer_id\").distinct()\n",
    "    \n",
    "    # Find buyers who bought S8 but not iPhone (anti join)\n",
    "    result_df = s8_buyers.join(iphone_buyers, on=\"buyer_id\", how=\"left_anti\")\n",
    "    \n",
    "    # Write the result into the target Hive table\n",
    "   # Write the result into the target Hive table, overwriting the existing table\n",
    "    result_df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(target_hive_table)\n",
    "\n",
    "\n",
    "    return target_hive_table \n",
    "\n",
    "# Prepare the data (buyers who bought S8 but not iPhone)\n",
    "data_preparation_api(spark, \"product_data\", \"sales_data\", \"buyers_s8_not_iphone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89734a0c-f6cb-471d-977b-63023dc225ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|buyer_id|\n+--------+\n|       1|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM buyers_s8_not_iphone\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Iphone Sales Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
